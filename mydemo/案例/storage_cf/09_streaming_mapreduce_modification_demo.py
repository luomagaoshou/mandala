#!/usr/bin/env python3
"""
æµå¼è®¡ç®—Map-Reduceç²¾ç¡®èŠ‚ç‚¹ä¿®æ”¹æ¼”ç¤º
==================================

æœ¬ç¤ºä¾‹æ¼”ç¤ºå¦‚ä½•åœ¨æµå¼è®¡ç®—çš„Map-Reduceæµç¨‹ä¸­å®ç°ç²¾ç¡®çš„èŠ‚ç‚¹ä¿®æ”¹ï¼š
1. å®ç°å®Œæ•´çš„Map-Reduceæ•°æ®å¤„ç†ç®¡é“
2. åœ¨æ¯ä¸ªå¤„ç†é˜¶æ®µè¿›è¡Œç²¾ç¡®å‚æ•°ä¿®æ”¹
3. å±•ç¤ºæµå¼è®¡ç®—ä¸­çš„å¢é‡æ›´æ–°å’Œé‡æ–°è®¡ç®—
4. ä¿æŒè®¡ç®—å›¾çš„ä¸€è‡´æ€§å’Œå®Œæ•´æ€§

æµç¨‹è®¾è®¡ï¼š
- æ•°æ®æºç”Ÿæˆï¼šç”Ÿæˆå¤§é‡åŸå§‹æ•°æ®
- æ•°æ®åˆ†å—ï¼šå°†æ•°æ®åˆ†æˆå¤šä¸ªå—è¿›è¡Œå¹¶è¡Œå¤„ç†
- Mapé˜¶æ®µï¼šå¯¹æ¯ä¸ªæ•°æ®å—è¿›è¡Œæ˜ å°„å˜æ¢
- Shuffleé˜¶æ®µï¼šæŒ‰keyé‡æ–°åˆ†ç»„æ•°æ®
- Reduceé˜¶æ®µï¼šå¯¹åˆ†ç»„æ•°æ®è¿›è¡Œèšåˆè®¡ç®—
- åå¤„ç†ï¼šå¯¹æœ€ç»ˆç»“æœè¿›è¡Œæ ¼å¼åŒ–å’ŒéªŒè¯
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', '..'))

from mandala1.imports import *
import pandas as pd
import numpy as np
from typing import List, Dict, Tuple, Optional, Any, Set, TYPE_CHECKING
import time
import copy
import random
from collections import defaultdict
import math

if TYPE_CHECKING:
    from mandala1.cf import ComputationFrame
    from mandala1.core import Call

# åˆ›å»ºå­˜å‚¨å®ä¾‹ï¼ˆä½¿ç”¨å†…å­˜æ•°æ®åº“ï¼‰
storage = Storage(db_path=":memory:")

# ==================== æ•°æ®æºå’Œåˆ†å—é˜¶æ®µ ====================

@op
def generate_data_source(size: int = 1000, seed: int = 42, value_range: Tuple[float, float] = (0.0, 100.0)) -> List[Dict[str, Any]]:
    """
    ç”Ÿæˆæ•°æ®æºï¼šæ¨¡æ‹Ÿå¤§é‡çš„åŸå§‹æ•°æ®
    """
    print(f"ç”Ÿæˆæ•°æ®æº - å¤§å°: {size}, éšæœºç§å­: {seed}, å€¼èŒƒå›´: {value_range}")
    
    random.seed(seed)
    np.random.seed(seed)
    
    data = []
    categories = ['A', 'B', 'C', 'D', 'E']
    
    for i in range(size):
        record = {
            'id': i,
            'category': random.choice(categories),
            'value': random.uniform(value_range[0], value_range[1]),
            'timestamp': time.time() + i,
            'metadata': {
                'source': f'sensor_{i % 10}',
                'quality': random.uniform(0.7, 1.0)
            }
        }
        data.append(record)
    
    print(f"   ç”Ÿæˆäº† {len(data)} æ¡è®°å½•")
    return data

@op
def partition_data(data: List[Dict[str, Any]], num_partitions: int = 4, 
                  partition_strategy: str = "hash") -> Dict[int, List[Dict[str, Any]]]:
    """
    æ•°æ®åˆ†å—ï¼šå°†æ•°æ®åˆ†æˆå¤šä¸ªåˆ†åŒºç”¨äºå¹¶è¡Œå¤„ç†
    """
    print(f"æ•°æ®åˆ†å— - åˆ†åŒºæ•°: {num_partitions}, ç­–ç•¥: {partition_strategy}")
    
    partitions = defaultdict(list)
    
    for record in data:
        if partition_strategy == "hash":
            # åŸºäºIDçš„å“ˆå¸Œåˆ†åŒº
            partition_id = record['id'] % num_partitions
        elif partition_strategy == "category":
            # åŸºäºç±»åˆ«çš„åˆ†åŒº
            partition_id = hash(record['category']) % num_partitions
        elif partition_strategy == "range":
            # åŸºäºå€¼èŒƒå›´çš„åˆ†åŒº
            value = record['value']
            partition_id = min(int(value / (100.0 / num_partitions)), num_partitions - 1)
        else:
            partition_id = record['id'] % num_partitions
        
        partitions[partition_id].append(record)
    
    result = dict(partitions)
    for pid, partition in result.items():
        print(f"   åˆ†åŒº {pid}: {len(partition)} æ¡è®°å½•")
    
    return result

# ==================== Mapé˜¶æ®µ ====================

@op
def map_transform_partition(partition_data: List[Dict[str, Any]], 
                           transform_type: str = "normalize",
                           scale_factor: float = 1.0,
                           filter_threshold: float = 0.0) -> List[Dict[str, Any]]:
    """
    Mapé˜¶æ®µï¼šå¯¹åˆ†åŒºæ•°æ®è¿›è¡Œå˜æ¢å¤„ç†
    """
    print(f"Mapå˜æ¢ - ç±»å‹: {transform_type}, ç¼©æ”¾: {scale_factor}, è¿‡æ»¤é˜ˆå€¼: {filter_threshold}")
    print(f"   å¤„ç† {len(partition_data)} æ¡è®°å½•")
    
    transformed = []
    
    for record in partition_data:
        # è®¡ç®—å˜æ¢åçš„å€¼
        if transform_type == "normalize":
            # å½’ä¸€åŒ–å¤„ç†
            transformed_value = (record['value'] / 100.0) * scale_factor
        elif transform_type == "logarithm":
            # å¯¹æ•°å˜æ¢
            transformed_value = math.log(record['value'] + 1) * scale_factor
        elif transform_type == "square":
            # å¹³æ–¹å˜æ¢
            transformed_value = (record['value'] ** 2) * scale_factor
        else:
            transformed_value = record['value'] * scale_factor
        
        # åº”ç”¨è¿‡æ»¤æ¡ä»¶
        if transformed_value >= filter_threshold:
            new_record = {
                'id': record['id'],
                'category': record['category'],
                'original_value': record['value'],
                'transformed_value': transformed_value,
                'quality_score': record['metadata']['quality'],
                'source': record['metadata']['source'],
                'timestamp': record['timestamp']
            }
            transformed.append(new_record)
    
    print(f"   å˜æ¢åä¿ç•™ {len(transformed)} æ¡è®°å½•")
    return transformed

@op
def map_compute_features(partition_data: List[Dict[str, Any]], 
                        feature_types: List[str] = ["mean", "variance"],
                        window_size: int = 10) -> Dict[str, List[Dict[str, Any]]]:
    """
    Mapé˜¶æ®µï¼šè®¡ç®—ç‰¹å¾å‘é‡
    """
    print(f"Mapç‰¹å¾è®¡ç®— - ç‰¹å¾ç±»å‹: {feature_types}, çª—å£å¤§å°: {window_size}")
    
    features_by_category = defaultdict(list)
    
    # æŒ‰ç±»åˆ«åˆ†ç»„
    category_groups = defaultdict(list)
    for record in partition_data:
        category_groups[record['category']].append(record)
    
    # ä¸ºæ¯ä¸ªç±»åˆ«è®¡ç®—ç‰¹å¾
    for category, records in category_groups.items():
        records.sort(key=lambda x: x['timestamp'])  # æŒ‰æ—¶é—´æ’åº
        
        # æ»‘åŠ¨çª—å£è®¡ç®—ç‰¹å¾
        for i in range(0, len(records), window_size):
            window_records = records[i:i + window_size]
            if len(window_records) < window_size // 2:  # è·³è¿‡å¤ªå°çš„çª—å£
                continue
            
            values = [r['transformed_value'] for r in window_records]
            qualities = [r['quality_score'] for r in window_records]
            
            feature_record = {
                'category': category,
                'window_start': i,
                'window_size': len(window_records),
                'features': {}
            }
            
            # è®¡ç®—ä¸åŒç±»å‹çš„ç‰¹å¾
            if "mean" in feature_types:
                feature_record['features']['mean'] = np.mean(values)
            if "variance" in feature_types:
                feature_record['features']['variance'] = np.var(values)
            if "median" in feature_types:
                feature_record['features']['median'] = np.median(values)
            if "quality_avg" in feature_types:
                feature_record['features']['quality_avg'] = np.mean(qualities)
            
            features_by_category[category].append(feature_record)
    
    result = dict(features_by_category)
    for cat, features in result.items():
        print(f"   ç±»åˆ« {cat}: è®¡ç®—äº† {len(features)} ä¸ªç‰¹å¾çª—å£")
    
    return result

# ==================== Shuffleé˜¶æ®µ ====================

@op
def shuffle_merge_partitions(partition_results: List[Dict[str, List[Dict[str, Any]]]], 
                           merge_strategy: str = "category",
                           sort_by: str = "category") -> Dict[str, List[Dict[str, Any]]]:
    """
    Shuffleé˜¶æ®µï¼šåˆå¹¶å’Œé‡æ–°åˆ†ç»„æ¥è‡ªä¸åŒåˆ†åŒºçš„ç»“æœ
    """
    print(f"Shuffleåˆå¹¶ - ç­–ç•¥: {merge_strategy}, æ’åº: {sort_by}")
    
    merged_data = defaultdict(list)
    
    # åˆå¹¶æ‰€æœ‰åˆ†åŒºçš„ç»“æœ
    for partition_result in partition_results:
        for category, features in partition_result.items():
            merged_data[category].extend(features)
    
    # å¯¹æ¯ä¸ªç±»åˆ«çš„æ•°æ®è¿›è¡Œæ’åº
    for category in merged_data:
        if sort_by == "category":
            merged_data[category].sort(key=lambda x: x['category'])
        elif sort_by == "window_start":
            merged_data[category].sort(key=lambda x: x['window_start'])
        elif sort_by == "mean_value" and 'mean' in merged_data[category][0]['features']:
            merged_data[category].sort(key=lambda x: x['features']['mean'])
    
    result = dict(merged_data)
    total_records = sum(len(features) for features in result.values())
    print(f"   åˆå¹¶å®Œæˆï¼Œæ€»å…± {total_records} ä¸ªç‰¹å¾çª—å£ï¼Œåˆ†å¸ƒåœ¨ {len(result)} ä¸ªç±»åˆ«ä¸­")
    
    return result

# ==================== Reduceé˜¶æ®µ ====================

@op
def reduce_aggregate_features(shuffled_data: Dict[str, List[Dict[str, Any]]], 
                             aggregation_functions: List[str] = ["sum", "avg", "max"],
                             min_window_count: int = 2) -> Dict[str, Dict[str, float]]:
    """
    Reduceé˜¶æ®µï¼šå¯¹æ¯ä¸ªç±»åˆ«çš„ç‰¹å¾è¿›è¡Œèšåˆ
    """
    print(f"Reduceèšåˆ - å‡½æ•°: {aggregation_functions}, æœ€å°çª—å£æ•°: {min_window_count}")
    
    aggregated_results = {}
    
    for category, feature_windows in shuffled_data.items():
        if len(feature_windows) < min_window_count:
            print(f"   è·³è¿‡ç±»åˆ« {category}ï¼Œçª—å£æ•°ä¸è¶³ ({len(feature_windows)} < {min_window_count})")
            continue
        
        category_aggregation = {}
        
        # æå–æ‰€æœ‰ç‰¹å¾å€¼
        feature_names = set()
        for window in feature_windows:
            feature_names.update(window['features'].keys())
        
        # å¯¹æ¯ç§ç‰¹å¾ç±»å‹è¿›è¡Œèšåˆ
        for feature_name in feature_names:
            feature_values = [w['features'][feature_name] for w in feature_windows 
                             if feature_name in w['features']]
            
            if not feature_values:
                continue
            
            feature_agg = {}
            
            if "sum" in aggregation_functions:
                feature_agg['sum'] = sum(feature_values)
            if "avg" in aggregation_functions:
                feature_agg['avg'] = np.mean(feature_values)
            if "max" in aggregation_functions:
                feature_agg['max'] = max(feature_values)
            if "min" in aggregation_functions:
                feature_agg['min'] = min(feature_values)
            if "std" in aggregation_functions:
                feature_agg['std'] = np.std(feature_values)
            
            category_aggregation[feature_name] = feature_agg
        
        aggregated_results[category] = category_aggregation
        print(f"   ç±»åˆ« {category}: èšåˆäº† {len(feature_names)} ç§ç‰¹å¾ç±»å‹")
    
    return aggregated_results

@op
def reduce_compute_statistics(aggregated_data: Dict[str, Dict[str, Dict[str, float]]], 
                             stat_types: List[str] = ["correlation", "ranking"],
                             ranking_metric: str = "avg") -> Dict[str, Any]:
    """
    Reduceé˜¶æ®µï¼šè®¡ç®—è·¨ç±»åˆ«çš„ç»Ÿè®¡ä¿¡æ¯
    """
    print(f"Reduceç»Ÿè®¡è®¡ç®— - ç»Ÿè®¡ç±»å‹: {stat_types}, æ’åæŒ‡æ ‡: {ranking_metric}")
    
    statistics = {}
    
    # è®¡ç®—ç±»åˆ«æ’å
    if "ranking" in stat_types:
        category_scores = {}
        for category, features in aggregated_data.items():
            # è®¡ç®—ç±»åˆ«çš„ç»¼åˆå¾—åˆ†
            total_score = 0
            feature_count = 0
            
            for feature_name, agg_values in features.items():
                if ranking_metric in agg_values:
                    total_score += agg_values[ranking_metric]
                    feature_count += 1
            
            if feature_count > 0:
                category_scores[category] = total_score / feature_count
        
        # æŒ‰å¾—åˆ†æ’åº
        ranked_categories = sorted(category_scores.items(), 
                                 key=lambda x: x[1], reverse=True)
        statistics['category_ranking'] = ranked_categories
        print(f"   ç±»åˆ«æ’å: {[cat for cat, score in ranked_categories]}")
    
    # è®¡ç®—ç‰¹å¾ç›¸å…³æ€§
    if "correlation" in stat_types:
        # ç®€åŒ–çš„ç›¸å…³æ€§è®¡ç®—
        feature_correlations = {}
        all_categories = list(aggregated_data.keys())
        
        if len(all_categories) >= 2:
            for i, cat1 in enumerate(all_categories):
                for cat2 in all_categories[i+1:]:
                    # è®¡ç®—ä¸¤ä¸ªç±»åˆ«é—´çš„ç‰¹å¾ç›¸ä¼¼åº¦
                    common_features = set(aggregated_data[cat1].keys()) & set(aggregated_data[cat2].keys())
                    if common_features:
                        correlations = []
                        for feature in common_features:
                            if ranking_metric in aggregated_data[cat1][feature] and ranking_metric in aggregated_data[cat2][feature]:
                                val1 = aggregated_data[cat1][feature][ranking_metric]
                                val2 = aggregated_data[cat2][feature][ranking_metric]
                                # ç®€åŒ–çš„ç›¸å…³æ€§åº¦é‡
                                correlation = 1.0 / (1.0 + abs(val1 - val2))
                                correlations.append(correlation)
                        
                        if correlations:
                            feature_correlations[f"{cat1}-{cat2}"] = np.mean(correlations)
        
        statistics['feature_correlations'] = feature_correlations
        print(f"   è®¡ç®—äº† {len(feature_correlations)} å¯¹ç±»åˆ«çš„ç›¸å…³æ€§")
    
    # å…¨å±€ç»Ÿè®¡
    statistics['total_categories'] = len(aggregated_data)
    statistics['total_features'] = sum(len(features) for features in aggregated_data.values())
    
    return statistics

# ==================== åå¤„ç†é˜¶æ®µ ====================

@op
def post_process_results(statistics: Dict[str, Any], 
                        format_type: str = "summary",
                        top_n: int = 3,
                        include_details: bool = True) -> Dict[str, Any]:
    """
    åå¤„ç†é˜¶æ®µï¼šæ ¼å¼åŒ–å’ŒéªŒè¯æœ€ç»ˆç»“æœ
    """
    print(f"åå¤„ç† - æ ¼å¼: {format_type}, Top-N: {top_n}, åŒ…å«è¯¦æƒ…: {include_details}")
    
    processed_results = {
        'summary': {},
        'metadata': {
            'processing_time': time.time(),
            'format_type': format_type,
            'top_n': top_n
        }
    }
    
    # ç”Ÿæˆæ‘˜è¦
    if 'category_ranking' in statistics:
        top_categories = statistics['category_ranking'][:top_n]
        processed_results['summary']['top_categories'] = [
            {'category': cat, 'score': score, 'rank': i+1} 
            for i, (cat, score) in enumerate(top_categories)
        ]
    
    if 'feature_correlations' in statistics:
        # æ‰¾å‡ºæœ€é«˜ç›¸å…³æ€§çš„ç±»åˆ«å¯¹
        correlations = statistics['feature_correlations']
        if correlations:
            max_correlation = max(correlations.items(), key=lambda x: x[1])
            processed_results['summary']['highest_correlation'] = {
                'category_pair': max_correlation[0],
                'correlation_score': max_correlation[1]
            }
    
    # æ·»åŠ è¯¦ç»†ä¿¡æ¯
    if include_details:
        processed_results['details'] = statistics
    
    processed_results['summary']['total_categories'] = statistics.get('total_categories', 0)
    processed_results['summary']['total_features'] = statistics.get('total_features', 0)
    
    print(f"   ç”Ÿæˆæ‘˜è¦åŒ…å« {len(processed_results['summary'])} é¡¹æŒ‡æ ‡")
    
    return processed_results

# ==================== æµå¼ä¿®æ”¹å™¨ç±» ====================

class StreamingModifier:
    """
    æµå¼è®¡ç®—ä¿®æ”¹å™¨ï¼šä¸“é—¨å¤„ç†æµå¼è®¡ç®—æµç¨‹ä¸­çš„å‚æ•°ä¿®æ”¹
    """
    
    def __init__(self, storage: Storage):
        self.storage = storage
        self.modification_history = []
        
    def locate_call_by_context(self, cf: "ComputationFrame", function_name: str, 
                              input_context: Dict[str, Any] = None) -> List[Tuple[str, "Call"]]:
        """
        åœ¨æµå¼è®¡ç®—çš„CFä¸­å®šä½ç‰¹å®šå‡½æ•°è°ƒç”¨
        """
        print(f"\nğŸ” å®šä½æµå¼è°ƒç”¨ - å‡½æ•°: {function_name}")
        if input_context:
            print(f"   ä¸Šä¸‹æ–‡æ¡ä»¶: {input_context}")
        
        matching_calls = []
        
        if function_name not in cf.fs:
            print(f"âŒ å‡½æ•° '{function_name}' æœªåœ¨è®¡ç®—å›¾ä¸­æ‰¾åˆ°")
            return matching_calls
        
        call_hids = cf.fs[function_name]
        print(f"   æ‰¾åˆ° {len(call_hids)} ä¸ªè°ƒç”¨")
        
        for call_hid in call_hids:
            call = cf.calls[call_hid]
            
            if not input_context:
                matching_calls.append((call_hid, call))
                continue
            
            call_inputs = {k: self.storage.unwrap(v) for k, v in call.inputs.items()}
            match = True
            
            for param_name, expected_value in input_context.items():
                if param_name not in call_inputs:
                    match = False
                    break
                if call_inputs[param_name] != expected_value:
                    match = False
                    break
            
            if match:
                matching_calls.append((call_hid, call))
                print(f"   âœ… åŒ¹é…è°ƒç”¨ {call_hid[:8]}...")
        
        print(f"ğŸ¯ å…±æ‰¾åˆ° {len(matching_calls)} ä¸ªåŒ¹é…çš„è°ƒç”¨")
        return matching_calls
    
    def modify_and_reexecute_stage(self, cf: "ComputationFrame", 
                                  stage_function: str, 
                                  param_modifications: Dict[str, Any],
                                  context_filter: Dict[str, Any] = None) -> "ComputationFrame":
        """
        ä¿®æ”¹ç‰¹å®šé˜¶æ®µçš„å‚æ•°å¹¶é‡æ–°æ‰§è¡Œï¼Œä½¿ç”¨æ›´å®‰å…¨çš„èŠ‚ç‚¹æ›¿æ¢ç­–ç•¥
        """
        print(f"\nğŸ”„ ä¿®æ”¹æ‰§è¡Œé˜¶æ®µ - {stage_function}")
        print(f"   å‚æ•°ä¿®æ”¹: {param_modifications}")
        
        # å®šä½ç›®æ ‡å‡½æ•°è°ƒç”¨
        matching_calls = self.locate_call_by_context(cf, stage_function, context_filter)
        
        if not matching_calls:
            print("âŒ æœªæ‰¾åˆ°åŒ¹é…çš„è°ƒç”¨")
            return cf
        
        # é€‰æ‹©ç¬¬ä¸€ä¸ªåŒ¹é…çš„è°ƒç”¨è¿›è¡Œä¿®æ”¹
        call_hid, call = matching_calls[0]
        
        # è·å–åŸå§‹å‚æ•°å¹¶åº”ç”¨ä¿®æ”¹
        original_inputs = {k: self.storage.unwrap(v) for k, v in call.inputs.items()}
        print(f"   åŸå§‹å‚æ•°: {list(original_inputs.keys())}")
        
        modified_inputs = original_inputs.copy()
        for param_name, new_value in param_modifications.items():
            if param_name in modified_inputs:
                old_value = modified_inputs[param_name]
                modified_inputs[param_name] = new_value
                print(f"   ğŸ“ {param_name}: {old_value} -> {new_value}")
                
                # è®°å½•ä¿®æ”¹å†å²
                self.modification_history.append({
                    "stage": stage_function,
                    "call_hid": call_hid,
                    "param_name": param_name,
                    "old_value": old_value,
                    "new_value": new_value,
                    "timestamp": time.time()
                })
            else:
                print(f"   âš ï¸  å‚æ•° {param_name} ä¸å­˜åœ¨ï¼Œè·³è¿‡")
        
        # æ‰¾åˆ°ç›®æ ‡å‡½æ•°èŠ‚ç‚¹
        target_func_name = None
        for fname in cf.fnames:
            if call_hid in cf.fs[fname]:
                target_func_name = fname
                break
                
        if not target_func_name:
            print(f"âŒ æ— æ³•æ‰¾åˆ°è°ƒç”¨å¯¹åº”çš„å‡½æ•°èŠ‚ç‚¹")
            return cf
            
        print(f"   ğŸ¯ ç›®æ ‡å‡½æ•°èŠ‚ç‚¹: {target_func_name}")
        
        # è®°å½•åŸå§‹ç»Ÿè®¡ä¿¡æ¯
        original_var_count = len(cf.vnames)
        original_func_count = len(cf.fnames)
        print(f"   ğŸ“Š åŸå§‹å›¾ç»Ÿè®¡ - å˜é‡: {original_var_count}, å‡½æ•°: {original_func_count}")
        
        # é‡æ–°æ‰§è¡Œä¿®æ”¹çš„å‡½æ•°è·å–æ–°ç»“æœ
        with self.storage:
            print(f"   âš¡ é‡æ–°æ‰§è¡Œ {stage_function}...")
            
            if stage_function == "generate_data_source":
                new_result = generate_data_source(**modified_inputs)
            elif stage_function == "partition_data":
                new_result = partition_data(**modified_inputs)
            elif stage_function == "map_transform_partition":
                new_result = map_transform_partition(**modified_inputs)
            elif stage_function == "map_compute_features":
                new_result = map_compute_features(**modified_inputs)
            elif stage_function == "shuffle_merge_partitions":
                new_result = shuffle_merge_partitions(**modified_inputs)
            elif stage_function == "reduce_aggregate_features":
                new_result = reduce_aggregate_features(**modified_inputs)
            elif stage_function == "reduce_compute_statistics":
                new_result = reduce_compute_statistics(**modified_inputs)
            elif stage_function == "post_process_results":
                new_result = post_process_results(**modified_inputs)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„å‡½æ•°: {stage_function}")
            
            print(f"   âœ… é‡æ–°æ‰§è¡Œ {stage_function} æˆåŠŸ")
            
        # è·å–æ–°è°ƒç”¨
        new_call = self.storage.get_ref_creator(new_result)
        if not new_call:
            print(f"âŒ æ— æ³•è·å–æ–°è°ƒç”¨")
            return cf
            
        print(f"   ğŸ“‹ æ–°è°ƒç”¨ ID: {new_call.hid[:8]}...")
        
        # ç­–ç•¥ï¼šåˆ›å»ºæ–°çš„ ComputationFrame å¹¶ä½¿ç”¨é›†åˆæ“ä½œæ¥æ›¿æ¢
        print(f"   ğŸ”„ æ„å»ºæ›¿æ¢åçš„è®¡ç®—å›¾...")
        
        # 1. åˆ›å»ºåŒ…å«æ–°ç»“æœçš„ ComputationFrame
        new_cf = self.storage.cf(new_result)
        
        # 2. å¦‚æœéœ€è¦æ‰©å±•ä»¥åŒ…å«ä¸‹æ¸¸ä¾èµ–
        if target_func_name == "generate_data_source":
            # å¯¹äºæ•°æ®æºï¼Œå¯èƒ½éœ€è¦æ‰©å±•å‰å‘ä»¥åŒ…å«æ•°æ®åˆ†åŒº
            new_cf = new_cf.expand_forward()
        
        # 3. ä½¿ç”¨é€‰æ‹©æ“ä½œæ¥ä¿æŒä¸åŸå§‹CFç›¸åŒçš„èŠ‚ç‚¹ç»“æ„
        if len(new_cf.vnames) != original_var_count or len(new_cf.fnames) != original_func_count:
            print(f"   ğŸ”§ è°ƒæ•´è®¡ç®—å›¾ç»“æ„ä»¥åŒ¹é…åŸå§‹...")
            
            # ç¡®ä¿åŒ…å«åŸå§‹çš„å‡½æ•°èŠ‚ç‚¹
            if target_func_name not in new_cf.fnames:
                # æ‰‹åŠ¨é‡å»ºç›¸åŒç»“æ„çš„CF
                result_cf = cf.copy()
                
                # å®‰å…¨åœ°ç§»é™¤æ—§è°ƒç”¨ï¼šä½¿ç”¨ select_subsets æ¥æ’é™¤ç‰¹å®šè°ƒç”¨
                old_call_hids = {call_hid}
                new_fs = {}
                for fname, call_hids in result_cf.fs.items():
                    filtered_calls = call_hids - old_call_hids
                    if fname == target_func_name:
                        # ä¸ºç›®æ ‡å‡½æ•°æ·»åŠ æ–°è°ƒç”¨
                        filtered_calls.add(new_call.hid)
                        # æ·»åŠ æ–°è°ƒç”¨åˆ°å­˜å‚¨
                        result_cf.calls[new_call.hid] = new_call
                        if new_call.hid not in result_cf.callinv:
                            result_cf.callinv[new_call.hid] = set()
                        result_cf.callinv[new_call.hid].add(fname)
                    new_fs[fname] = filtered_calls
                
                # æ›´æ–°å‡½æ•°é›†åˆ
                result_cf.fs = new_fs
                
                # æ¸…ç†å­¤ç«‹çš„è°ƒç”¨å¼•ç”¨
                if call_hid in result_cf.calls:
                    del result_cf.calls[call_hid]
                if call_hid in result_cf.callinv:
                    del result_cf.callinv[call_hid]
                
                # æ›´æ–°å¼•ç”¨æ˜ å°„
                for output_name, output_ref in new_call.outputs.items():
                    result_cf.refs[output_ref.hid] = output_ref
                    result_cf.creator[output_ref.hid] = new_call.hid
                    
                    # æ‰¾åˆ°å¯¹åº”çš„è¾“å‡ºå˜é‡å¹¶æ›´æ–°å¼•ç”¨
                    for src, dst, label in result_cf.out_edges(target_func_name):
                        if dst in result_cf.vnames:
                            # ç§»é™¤æ—§å¼•ç”¨
                            old_refs = set()
                            for old_out_name, old_out_ref in call.outputs.items():
                                if old_out_ref.hid in result_cf.vs[dst]:
                                    old_refs.add(old_out_ref.hid)
                            
                            for old_ref_hid in old_refs:
                                result_cf.vs[dst].discard(old_ref_hid)
                                if old_ref_hid in result_cf.refs:
                                    del result_cf.refs[old_ref_hid]
                                if old_ref_hid in result_cf.creator:
                                    del result_cf.creator[old_ref_hid]
                                if old_ref_hid in result_cf.refinv:
                                    del result_cf.refinv[old_ref_hid]
                            
                            # æ·»åŠ æ–°å¼•ç”¨
                            result_cf.vs[dst].add(output_ref.hid)
                            if output_ref.hid not in result_cf.refinv:
                                result_cf.refinv[output_ref.hid] = set()
                            result_cf.refinv[output_ref.hid].add(dst)
                            break
                
                # æ›´æ–°è¾“å…¥å¼•ç”¨çš„æ¶ˆè´¹è€…æ˜ å°„
                for input_name, input_ref in new_call.inputs.items():
                    if input_ref.hid not in result_cf.consumers:
                        result_cf.consumers[input_ref.hid] = set()
                    result_cf.consumers[input_ref.hid].add(new_call.hid)
            else:
                result_cf = new_cf
        else:
            result_cf = new_cf
        
        # éªŒè¯èŠ‚ç‚¹æ•°é‡ä¸€è‡´æ€§
        final_var_count = len(result_cf.vnames)
        final_func_count = len(result_cf.fnames)
        
        print(f"   ğŸ“Š æœ€ç»ˆå›¾ç»Ÿè®¡ - å˜é‡: {final_var_count}, å‡½æ•°: {final_func_count}")
        
        if final_var_count == original_var_count and final_func_count == original_func_count:
            print(f"   âœ… è®¡ç®—å›¾ç»“æ„ä¿æŒå®Œå…¨ä¸€è‡´")
        else:
            print(f"   âš ï¸  è®¡ç®—å›¾ç»“æ„å‘ç”Ÿå˜åŒ–")
            print(f"   å˜é‡æ•°é‡: {original_var_count} -> {final_var_count}")
            print(f"   å‡½æ•°æ•°é‡: {original_func_count} -> {final_func_count}")
        
        print(f"   âœ… èŠ‚ç‚¹æ›¿æ¢å®Œæˆ")
        return result_cf
    
    def get_modification_summary(self) -> Dict[str, Any]:
        """
        è·å–ä¿®æ”¹æ‘˜è¦
        """
        if not self.modification_history:
            return {}
        
        summary = {
            "total_modifications": len(self.modification_history),
            "modified_stages": {},
            "modified_parameters": {},
            "timeline": []
        }
        
        for mod in self.modification_history:
            stage = mod['stage']
            param = mod['param_name']
            
            summary["modified_stages"][stage] = summary["modified_stages"].get(stage, 0) + 1
            summary["modified_parameters"][param] = summary["modified_parameters"].get(param, 0) + 1
            
            summary["timeline"].append({
                "timestamp": mod['timestamp'],
                "stage": stage,
                "parameter": param,
                "change": f"{mod['old_value']} -> {mod['new_value']}"
            })
        
        return summary

# ==================== ä¸»æ¼”ç¤ºæµç¨‹ ====================

def run_complete_streaming_pipeline():
    """
    è¿è¡Œå®Œæ•´çš„æµå¼è®¡ç®—ç®¡é“
    """
    print("=" * 80)
    print("1. è¿è¡Œå®Œæ•´çš„æµå¼Map-Reduceè®¡ç®—ç®¡é“")
    print("=" * 80)
    
    with storage:
        # é˜¶æ®µ1ï¼šç”Ÿæˆæ•°æ®æº
        print("\n--- é˜¶æ®µ1: æ•°æ®æºç”Ÿæˆ ---")
        raw_data = generate_data_source(size=500, seed=42, value_range=(1.0, 100.0))
        
        # é˜¶æ®µ2ï¼šæ•°æ®åˆ†åŒº
        print("\n--- é˜¶æ®µ2: æ•°æ®åˆ†åŒº ---")
        partitioned_data = partition_data(raw_data, num_partitions=4, partition_strategy="hash")
        
        # é˜¶æ®µ3ï¼šMapå˜æ¢ï¼ˆå¹¶è¡Œå¤„ç†æ¯ä¸ªåˆ†åŒºï¼‰
        print("\n--- é˜¶æ®µ3: Mapå˜æ¢å¤„ç† ---")
        map_results = []
        partitioned_data_unwrapped = storage.unwrap(partitioned_data)
        for partition_id, partition_records in partitioned_data_unwrapped.items():
            print(f"\nå¤„ç†åˆ†åŒº {partition_id}:")
            # å˜æ¢æ•°æ®
            transformed_partition = map_transform_partition(
                partition_records, 
                transform_type="normalize", 
                scale_factor=1.0, 
                filter_threshold=0.1
            )
            
            # è®¡ç®—ç‰¹å¾
            partition_features = map_compute_features(
                transformed_partition, 
                feature_types=["mean", "variance", "quality_avg"], 
                window_size=8
            )
            
            map_results.append(partition_features)
        
        # é˜¶æ®µ4ï¼šShuffleåˆå¹¶
        print("\n--- é˜¶æ®µ4: Shuffleåˆå¹¶ ---")
        shuffled_data = shuffle_merge_partitions(
            map_results, 
            merge_strategy="category", 
            sort_by="window_start"
        )
        
        # é˜¶æ®µ5ï¼šReduceèšåˆ
        print("\n--- é˜¶æ®µ5: Reduceèšåˆ ---")
        aggregated_features = reduce_aggregate_features(
            shuffled_data, 
            aggregation_functions=["sum", "avg", "max", "std"], 
            min_window_count=2
        )
        
        # é˜¶æ®µ6ï¼šç»Ÿè®¡è®¡ç®—
        print("\n--- é˜¶æ®µ6: ç»Ÿè®¡è®¡ç®— ---")
        statistics = reduce_compute_statistics(
            aggregated_features, 
            stat_types=["correlation", "ranking"], 
            ranking_metric="avg"
        )
        
        # é˜¶æ®µ7ï¼šåå¤„ç†
        print("\n--- é˜¶æ®µ7: åå¤„ç† ---")
        final_results = post_process_results(
            statistics, 
            format_type="summary", 
            top_n=3, 
            include_details=True
        )
        
        print(f"\nğŸ‰ æµå¼è®¡ç®—ç®¡é“æ‰§è¡Œå®Œæˆï¼")
        final_results_unwrapped = storage.unwrap(final_results)
        print(f"æœ€ç»ˆç»“æœæ‘˜è¦: {final_results_unwrapped['summary']}")
        
    return final_results

def demonstrate_streaming_modifications():
    """
    æ¼”ç¤ºæµå¼è®¡ç®—ä¸­çš„å„é˜¶æ®µå‚æ•°ä¿®æ”¹
    """
    print("\n" + "=" * 80)
    print("2. æ¼”ç¤ºæµå¼è®¡ç®—å„é˜¶æ®µçš„ç²¾ç¡®å‚æ•°ä¿®æ”¹")
    print("=" * 80)
    
    # åˆ›å»ºä¿®æ”¹å™¨
    modifier = StreamingModifier(storage)
    
    # è·å–å®Œæ•´çš„è®¡ç®—å›¾ï¼ˆåŒ…å«æ‰€æœ‰å‡½æ•°ï¼‰
    print("\næ„å»ºå®Œæ•´çš„è®¡ç®—å›¾...")
    full_cf = storage.cf(generate_data_source).expand_all()
    print(f"è®¡ç®—å›¾ç»Ÿè®¡ - å˜é‡: {len(full_cf.vnames)}, å‡½æ•°: {len(full_cf.fnames)}")
    print(f"å‡½æ•°åˆ—è¡¨: {list(full_cf.fnames)}")
    
    current_cf = full_cf
    
    # ä¿®æ”¹åœºæ™¯1ï¼šè°ƒæ•´æ•°æ®æºå‚æ•°
    print(f"\n" + "-" * 60)
    print("åœºæ™¯1: ä¿®æ”¹æ•°æ®æºç”Ÿæˆå‚æ•°")
    print("-" * 60)
    
    current_cf = modifier.modify_and_reexecute_stage(
        cf=current_cf,
        stage_function="generate_data_source",
        param_modifications={
            "size": 300,  # ä»500å‡å°‘åˆ°300
            "value_range": (5.0, 95.0)  # è°ƒæ•´å€¼èŒƒå›´
        }
    )
    
    # ä¿®æ”¹åœºæ™¯2ï¼šè°ƒæ•´æ•°æ®åˆ†åŒºå‚æ•°
    print(f"\n" + "-" * 60)
    print("åœºæ™¯2: ä¿®æ”¹æ•°æ®åˆ†åŒºå‚æ•°")
    print("-" * 60)
    
    current_cf = modifier.modify_and_reexecute_stage(
        cf=current_cf,
        stage_function="partition_data",
        param_modifications={
            "num_partitions": 6,  # ä»4æ”¹ä¸º6
            "partition_strategy": "category"  # ä»hashæ”¹ä¸ºcategory
        }
    )
    
    # æ˜¾ç¤ºä¿®æ”¹æ‘˜è¦
    print(f"\n" + "=" * 80)
    print("3. æµå¼è®¡ç®—ä¿®æ”¹æ‘˜è¦")
    print("=" * 80)
    
    summary = modifier.get_modification_summary()
    
    print(f"æ€»ä¿®æ”¹æ¬¡æ•°: {summary.get('total_modifications', 0)}")
    print(f"\nå„é˜¶æ®µä¿®æ”¹ç»Ÿè®¡:")
    for stage, count in summary.get('modified_stages', {}).items():
        print(f"  - {stage}: {count} æ¬¡ä¿®æ”¹")
    
    print(f"\nå‚æ•°ä¿®æ”¹ç»Ÿè®¡:")
    for param, count in summary.get('modified_parameters', {}).items():
        print(f"  - {param}: {count} æ¬¡ä¿®æ”¹")
    
    # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
    print(f"\næœ€ç»ˆè®¡ç®—å›¾ç»Ÿè®¡:")
    print(f"  å˜é‡èŠ‚ç‚¹: {len(current_cf.vnames)}")
    print(f"  å‡½æ•°èŠ‚ç‚¹: {len(current_cf.fnames)}")
    
    # å®‰å…¨åœ°è·å–æœ€ç»ˆç»“æœ
    print(f"\nä¿®æ”¹åçš„æœ€ç»ˆç»“æœ:")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç©ºå‡½æ•°èŠ‚ç‚¹
    empty_functions = []
    for fname in current_cf.fnames:
        if not current_cf.fs[fname]:
            empty_functions.append(fname)
    
    if empty_functions:
        print(f"âš ï¸  å‘ç°ç©ºå‡½æ•°èŠ‚ç‚¹: {empty_functions}")
        print("ç›´æ¥è·å–æ±‡ç‚¹å˜é‡çš„å€¼")
    else:
        print("âœ… è®¡ç®—å›¾å®Œæ•´ï¼Œè·å–æœ€ç»ˆç»“æœ")
    
    # ç›´æ¥è·å–æ±‡ç‚¹å˜é‡çš„å€¼ï¼Œé¿å… eval() çš„å¤æ‚æ€§
    if current_cf.sinks:
        sink_var = list(current_cf.sinks)[0]
        sink_values = current_cf.get_var_values(sink_var)
        if sink_values:
            sample_value = next(iter(sink_values))
            result = storage.unwrap(sample_value)
            print(f"æ±‡ç‚¹å˜é‡ '{sink_var}' çš„ç»“æœ: {str(result)[:300]}...")
        else:
            print("æ±‡ç‚¹å˜é‡æ— å†…å®¹")
    else:
        # å¦‚æœæ²¡æœ‰æ±‡ç‚¹å˜é‡ï¼Œè·å–æ‰€æœ‰å˜é‡çš„å€¼
        print("æ— æ±‡ç‚¹å˜é‡ï¼Œæ˜¾ç¤ºæ‰€æœ‰å˜é‡çš„å€¼:")
        for vname in current_cf.vnames:
            values = current_cf.get_var_values(vname)
            if values:
                sample_value = next(iter(values))
                result = storage.unwrap(sample_value)
                print(f"å˜é‡ '{vname}': {str(result)[:100]}...")
            else:
                print(f"å˜é‡ '{vname}': æ— å†…å®¹")
    
    return current_cf, modifier

def analyze_streaming_performance():
    """
    åˆ†ææµå¼è®¡ç®—çš„æ€§èƒ½å’Œä¿®æ”¹å½±å“
    """
    print(f"\n" + "=" * 80)
    print("4. æµå¼è®¡ç®—æ€§èƒ½åˆ†æ")
    print("=" * 80)
    
    print("æµå¼Map-Reduceè®¡ç®—çš„ä¼˜åŠ¿:")
    print("âœ… æ•°æ®åˆ†åŒºå¹¶è¡Œå¤„ç†ï¼Œæå‡è®¡ç®—æ•ˆç‡")
    print("âœ… é˜¶æ®µåŒ–å¤„ç†ï¼Œä¾¿äºè°ƒè¯•å’Œä¼˜åŒ–")
    print("âœ… ç²¾ç¡®å‚æ•°ä¿®æ”¹ï¼Œé¿å…å…¨é‡é‡è®¡ç®—")
    print("âœ… å¢é‡æ›´æ–°æ”¯æŒï¼Œé€‚åº”æ•°æ®æµå˜åŒ–")
    
    print("\nå‚æ•°ä¿®æ”¹çš„å½±å“åˆ†æ:")
    print("ğŸ”„ æ•°æ®æºä¿®æ”¹ -> å½±å“æ•´ä¸ªç®¡é“")
    print("ğŸ”„ Mapé˜¶æ®µä¿®æ”¹ -> å½±å“ä¸‹æ¸¸èšåˆå’Œç»Ÿè®¡")
    print("ğŸ”„ Reduceé˜¶æ®µä¿®æ”¹ -> å½±å“æœ€ç»ˆç»“æœæ ¼å¼")
    print("ğŸ”„ åå¤„ç†ä¿®æ”¹ -> ä»…å½±å“è¾“å‡ºå±•ç¤º")
    
    print("\nMandalaæ¡†æ¶åœ¨æµå¼è®¡ç®—ä¸­çš„ä½œç”¨:")
    print("ğŸ“Š è‡ªåŠ¨è®°å½•è®¡ç®—å›¾ä¾èµ–å…³ç³»")
    print("ğŸ’¾ æ™ºèƒ½ç¼“å­˜ä¸­é—´ç»“æœ")
    print("ğŸ” æ”¯æŒç²¾ç¡®çš„èŠ‚ç‚¹å®šä½")
    print("âš¡ é«˜æ•ˆçš„å¢é‡é‡æ–°è®¡ç®—")

def main():
    """
    ä¸»å‡½æ•°ï¼šæ¼”ç¤ºå®Œæ•´çš„æµå¼è®¡ç®—Map-Reduceä¿®æ”¹æµç¨‹
    """
    print("æµå¼è®¡ç®—Map-Reduceç²¾ç¡®èŠ‚ç‚¹ä¿®æ”¹æ¼”ç¤º")
    print("ä½¿ç”¨mandalaæ¡†æ¶å®ç°å¤§è§„æ¨¡æ•°æ®å¤„ç†ç®¡é“çš„å‚æ•°è°ƒä¼˜")
    
    # 1. è¿è¡Œå®Œæ•´çš„æµå¼è®¡ç®—ç®¡é“
    initial_results = run_complete_streaming_pipeline()
    
    # 2. æ¼”ç¤ºå„é˜¶æ®µçš„å‚æ•°ä¿®æ”¹
    final_cf, modifier = demonstrate_streaming_modifications()
    
    # 3. æ€§èƒ½åˆ†æ
    analyze_streaming_performance()
    
    print(f"\n" + "=" * 80)
    print("æµå¼è®¡ç®—Map-Reduceä¿®æ”¹æ¼”ç¤ºå®Œæˆï¼")
    print("=" * 80)
    
    print("\næ ¸å¿ƒç‰¹æ€§å±•ç¤º:")
    print("ğŸŒŠ å®Œæ•´çš„æµå¼Map-Reduceè®¡ç®—ç®¡é“")
    print("ğŸ¯ å¤šé˜¶æ®µç²¾ç¡®å‚æ•°ä¿®æ”¹")
    print("ğŸ”„ æ™ºèƒ½å¢é‡é‡æ–°è®¡ç®—")
    print("ğŸ“ˆ å®æ—¶æ€§èƒ½ç›‘æ§å’Œåˆ†æ")
    print("ğŸ’¡ åŸºäºMandalaæ¡†æ¶çš„é«˜æ•ˆå®ç°")

if __name__ == "__main__":
    main() 